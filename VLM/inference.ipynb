{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b84f946-b56d-4248-bc7f-b70ba488202d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionLanguageModel(\n",
       "  (vision_encoder): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(392, 768)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (text_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(30522, 768)\n",
       "  (fc_out): Linear(in_features=768, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the saved model (without DDP)\n",
    "model = torch.load(\"vision_language_model_full_v4_15.pth\", map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model (without DDP) and move it to the device (GPU if available)\n",
    "#model = torch.load(\"vision_language_model_full_v3_1.pth\", map_location=device)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b33b1c91-6e3f-47f5-a976-07e5cb65e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the preprocessing pipeline (same as used during training)\n",
    "preprocess_image = transforms.Compose([\n",
    "    transforms.Resize((392, 196)),  # Resize the image to match the model's input size\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# Function to preprocess a single image\n",
    "def preprocess_input_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure it's in RGB format\n",
    "    pixel_values = preprocess_image(image).unsqueeze(0)  # Add a batch dimension\n",
    "    return pixel_values\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"Filtered_Outfits/14708/complete.jpg\"\n",
    "pixel_values = preprocess_input_image(image_path)\n",
    "\n",
    "#14708, 2608 on 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6404a710-1677-4f96-aba6-09f0653b5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Load the tokenizer used during training\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def generate_caption(model, pixel_values, tokenizer, max_length=392):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Start with the [CLS] token (the beginning of a sequence)\n",
    "    generated_ids = torch.tensor([[tokenizer.cls_token_id]], device=pixel_values.device)  # Ensure it is on the same device\n",
    "\n",
    "    \n",
    "    for _ in range(max_length - 1):  # Generate tokens up to max_length\n",
    "        # Pad the current sequence to match the vision features' sequence length\n",
    "        input_ids = F.pad(generated_ids, (0, max_length - generated_ids.size(1)), value=tokenizer.pad_token_id)\n",
    "        \n",
    "        # Embed the padded input tokens\n",
    "        embedded_input_ids = model.token_embedding(input_ids)\n",
    "        \n",
    "        # Get vision features from the image\n",
    "        vision_features = model.vision_encoder(pixel_values)\n",
    "        \n",
    "        # Generate output using text decoder\n",
    "        outputs = model.text_decoder(embedded_input_ids, vision_features)\n",
    "        \n",
    "        logits = model.fc_out(outputs)\n",
    "        \n",
    "        # Get the next token prediction (argmax over the vocabulary dimension)\n",
    "        next_token_id = logits.argmax(dim=-1)[:, generated_ids.size(1) - 1].unsqueeze(0)  # Last token\n",
    "        print(next_token_id)\n",
    "        \n",
    "        # Append the generated token to the sequence\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        current_output = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "        print(f\"Current Generated Output: {current_output}\")\n",
    "        \n",
    "        # Stop generation if the [SEP] token is produced\n",
    "        if next_token_id.item() == tokenizer.sep_token_id or next_token_id.item() == 0:\n",
    "            break\n",
    "    \n",
    "    # Decode the generated token IDs back into a readable caption\n",
    "    generated_caption = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "    \n",
    "    return generated_caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7c83d35-667f-4f57-8b60-ab07c8f0bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1045]])\n",
      "Current Generated Output: i\n",
      "tensor([[1005]])\n",
      "Current Generated Output: i '\n",
      "tensor([[1049]])\n",
      "Current Generated Output: i'm\n",
      "tensor([[2559]])\n",
      "Current Generated Output: i'm looking\n",
      "tensor([[2005]])\n",
      "Current Generated Output: i'm looking for\n",
      "tensor([[1037]])\n",
      "Current Generated Output: i'm looking for a\n",
      "tensor([[10017]])\n",
      "Current Generated Output: i'm looking for a casual\n",
      "tensor([[1998]])\n",
      "Current Generated Output: i'm looking for a casual and\n",
      "tensor([[6625]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable\n",
      "tensor([[11018]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit\n",
      "tensor([[2005]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for\n",
      "tensor([[1037]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a\n",
      "tensor([[10017]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a casual\n",
      "tensor([[2154]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a casual day\n",
      "tensor([[2041]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a casual day out\n",
      "tensor([[2007]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a casual day out with\n",
      "tensor([[2814]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a casual day out with friends\n",
      "tensor([[1012]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a casual day out with friends.\n",
      "tensor([[102]])\n",
      "Current Generated Output: i'm looking for a casual and comfortable outfit for a casual day out with friends.\n",
      "Generated Caption: i'm looking for a casual and comfortable outfit for a casual day out with friends.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate the caption\n",
    "caption = generate_caption(model, pixel_values, tokenizer)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a0c5c-0a26-4d69-b9ab-a57174e4ff3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
